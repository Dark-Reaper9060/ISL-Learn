--------------------------------------------------------------------------------
PART 1: PROJECT INFORMATION
--------------------------------------------------------------------------------

## Title of Accessibility Problem Statement
Bridging the Communication Gap: AI-Powered Interactive Indian Sign Language (ISL) Learning Platform

## Brief Description of Accessibility Problem Statement
Millions of deaf and hard-of-hearing individuals in India face significant communication barriers due to the general public's limited knowledge of Indian Sign Language (ISL). Traditional methods of learning ISL, such as physical classes or static video tutorials, often lack accessibility, scalability, and, most importantly, immediate feedback. Learners watching static videos struggle to determine if they are mirroring the signs correctly, leading to incorrect practice and low retention rates. This educational gap perpetuates social isolation for the Deaf community and limits opportunities for inclusive interaction in hearing-dominant environments.

## Brief Description of Solution
SignSiksha is a browser-based, interactive learning platform that democratizes ISL education through Artificial Intelligence. It solves the "feedback loop" problem by using computer vision to act as a personal, virtual instructor.

By leveraging a standard webcam and advanced deep learning models (MediaPipe and Custom CNNs), SignSiksha tracks the user's hand movements in real-time. It compares the user's gesture against a database of ISL signs and provides instant visual correctionsâ€”visualized as success indicators and confidence bars. This allows learners to correct their form immediately.

The platform gamifies the experience with "Alphabet Blitz" and "Word Master" quizzes, making learning engaging and measurable. By removing the need for specialized hardware or in-person tutors, SignSiksha makes effective ISL learning accessible to anyone with an internet connection.

## Explanation of Details
SignSiksha integrates a modern, responsive frontend with a robust AI backend to deliver a seamless educational experience.

### 1. Technology Stack
-   Frontend: Built with React, TypeScript, and Tailwind CSS, providing a WCAG-compliant, accessible user interface. It manages camera access and visualizes feedback.
-   Backend: A Python FastAPI server backed by PostgreSQL. It processes video frames using MediaPipe(for hand landmark extraction) and TensorFlow/Keras (for sign classification), and manages user data securely.

### 2. Core Features
-   Real-Time Assessment: The system analyzes video frames at ~10 FPS, providing instantaneous feedback. A unique "Confidence Meter" shows users exactly how accurate their sign is (e.g., "85% match").
-   User Profiles & Analytics: Learners can create personal accounts to save their progress. The system tracks quiz history, calculating statistics like "Best Alphabet Score" and "Total Quizzes Taken" to visualize improvement over time.
-   Dual-Stream AI Model: To ensure high accuracy, the underlying model analyzes both the raw image of the hand and the geometric skeleton of the fingers. This makes it robust against varied lighting and backgrounds.
-   Interactive Quizzes:
    -   Alphabet Quiz: A timed mode to practice individual letters (A-Z).
    -   Word Quiz: A sequential mode where users must spell entire words (e.g., H-E-L-L-O), implementing a stability check (must hold sign for 1.5s) to ensure mastery.
-   Progress Tracking: A robust database system persists user scores across sessions, ensuring that learning milestones are never lost. Using PostgreSQL, we securely store user credentials and performance metrics.


--------------------------------------------------------------------------------
PART 2: PROJECT ARCHITECTURE
--------------------------------------------------------------------------------

# Project Analysis: SignSiksha

## Overview
SignSiksha is an AI-powered educational platform designed to teach Indian Sign Language (ISL). It uses a modern web stack for the frontend and a Python-based deep learning backend for real-time sign recognition, now integrated with a PostgreSQL database for comprehensive user management and progress tracking.

## System Architecture

### 1. Frontend (Client-Side)
-   Tech Stack: 
    -   Framework: React (Vite)
    -   Language: TypeScript
    -   Styling: Tailwind CSS + shadcn/ui
    -   State Management: React Context (Auth) + React Query
    -   Runtime: Browser
-   Key Components:
    -   `AuthContext.tsx`: Manages user authentication state, session persistence, and login/logout functionality.
    -   `CameraPreview.tsx`: Manages the user's webcam feed, capturing frames and converting them for WebSocket transmission.
    -   `useWebSocket.ts`: Handles the resilient WebSocket connection to the AI inference engine.
    -   `ProtectedRoute.tsx`: A higher-order component that secures private routes from unauthenticated access.
    -   Assessment Module:
        -   `Assessment.tsx`: Orchestrates the quiz flow, fetching high scores from the API for users .
        -   `AlphabetQuiz.tsx` & `WordQuiz.tsx`: Interactive testing modules with real-time AI feedback and stability checks.
    -   User Module:
        -   `Login.tsx`: Handles user registration and authentication.
        -   `Profile.tsx`: Visualizes user statistics, total quizzes taken, and best scores using data fetched from the backend.

### 2. Backend (Server-Side)
-   Tech Stack:
    -   Framework: FastAPI (Python)
    -   Database: PostgreSQL (with SQLAlchemy ORM)
    -   ML Libraries: TensorFlow/Keras, MediaPipe, OpenCV
    -   Security: Passlib (pbkdf2_sha256) for password hashing
-   Key Components:
    -   `app.py`: The application entry point, integrating the AI WebSocket endpoint (`/ws/predict`) with the REST API router.
    -   `database.py`: Manages the PostgreSQL connection pool and session lifecycle.
    -   `routes.py`: Defines REST endpoints for User Registration, Login, Score Submission, and Profile Retrieval.
    -   `models.py`: Defines the SQL schema for `User` (credentials, metadata) and `Score` (quiz results, timestamps).
    -   AI Engine:
        -   MediaPipe HandLandmarker: Extracts 21 3D hand landmarks and computes bounding boxes.
        -   Dual-Stream Model: Fuses raw image data (EfficientNet) with skeletal landmark data to predict ISL signs with high robustness against noise.

## Data Flow Pipeline

1.  Capture: Browser captures webcam video stream.
2.  Transmission: Video frames are sent via WebSocket to the inference engine.
3.  Inference: The Dual-Stream model predicts the sign class and confidence score.
4.  Feedback: The frontend receives the prediction and updates the active confidence meter.
5.  Scoring & Persistence:
    -   Upon quiz completion, the final score is calculated.
    -   Guest: Score is saved to local storage.
    -   User: `Assessment.tsx` POSTs the score to the `/scores/` API endpoint, where it is securely stored in PostgreSQL linked to the unique `user_id`.

## Key Files
-   Backend: `app.py`, `models.py`, `routes.py`, `crud.py`.
-   Frontend: `App.tsx`, `AuthContext.tsx`, `Assessment.tsx`, `Profile.tsx`.
